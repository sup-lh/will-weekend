<style> g{ font-size:26px; color: #27408B; background: #87CEFF; border-radius: 5px; opacity: 85%; } </style>

#
# **<g>readyï¼Ÿ</g>**


?> é˜¿å·´é˜¿å·´é˜¿å·´é˜¿å·´é˜¿å·´ ğŸ’¤ğŸ’¤

<br>

?> é˜¿å·´é˜¿å·´é˜¿å·´é˜¿å·´é˜¿å·´ ğŸ”‡ğŸ”‡

<br>

# **go!** 
<br>

?> æ–°å»ºä¸€ä¸ªæ–‡ä»¶å¤¹ï¼Œvs<br>
æ‰“å¼€é›†æˆç»ˆç«¯ï¼Œè¾“å…¥<br>
scrapy -h<br>
çœ‹åˆ°åé¢startprojectå¤åˆ¶ç²˜è´´è¾“å…¥ï¼š<br>
scrapy startproject æ–‡ä»¶å<br>

**åˆ›å»ºæ™®é€šscrapyæ¶æ„**<br>

?> cd è¿›spideræ–‡ä»¶ä¸­<br>
scrapy genspider example example.com<br>

**åˆ›å»ºçˆ¬è™«scrapyæ¶æ„**<br>

?> cd è¿›spideræ–‡ä»¶ä¸­<br>
scrapy genspider -t crawl example example.com<br>

***å¯åŠ¨çˆ¬è™«æ–‡ä»¶***<br>

?> cd è¿›spideræ–‡ä»¶<br>
scrapy crawl quotes



<br><br><br><br>

# çˆ¬è™«æ–‡ä»¶ ğŸŠğŸŠ
- **çˆ¬è™«æ–‡ä»¶é‡Œé¢çš„start_requestså‡½æ•°ä¼šç¬¬ä¸€æ¬¡æ‰§è¡Œï¼Œæ‰€ä»¥å°±ç®—å‡½æ•°å¤–é¢çš„start_urlså˜é‡é‡Œé¢æœ‰å¤šä¸ªurlï¼Œåœ¨æœ‰start_requestsçš„æƒ…å†µä¸‹ä¹Ÿä¼šå¤±æ•ˆ**
```python
    def start_requests(self):
        urls = [
            'http://www.baidu.com',
            'http://www.baidu.com',
        ]
        for url in urls:
            yield scrapy.Request(url=url, callback=self.parse)
```
**yieldçš„ä½œç”¨ï¼š**<br>

?> ç±»ä¼¼äºreturnï¼Œä½†æ˜¯returnä¼šç›´æ¥è·³å‡ºï¼Œå½“å†æ¬¡æ‰§è¡Œå¸¦æœ‰returnçš„å‡½æ•°æ—¶ä¼šä»å¤´è¿è¡Œï¼Œè€Œyieldåˆ™æœ‰æŒ‚èµ·çš„ä½œç”¨ï¼Œå½“å†æ¬¡æ‰§è¡Œæ—¶ä¼šä»yieldä¸­ç»§ç»­æ‰§è¡Œ<br>åé¢çš„parseä¸ç”¨åŠ æ‹¬å·
```python
urls = [
        'http://www.baidu.com',
        'http://www.baidu.com',
]
for url in urls:
        yield scrapy.Request(url=url, callback=self.parse)
```


**åœ¨parseå‡½æ•°é‡Œé¢æ‰“çˆ¬è™«ä»£ç **<br>

?> ç¬¬ä¸€æ¬¡è¯·æ±‚å·²ç»åœ¨start_requestså‡½æ•°é‡Œé¢å·²ç»å¾—åˆ°ï¼Œparseå‡½æ•°æ‰€å¸¦çš„responseå°±æ˜¯html<br>
```python
 def parse(self, response):
```

**è¦<br>from æ–‡ä»¶å.items import æ–‡ä»¶åItem<br>**
```python
class QuotesSpider(scrapy.Spider):
```

**å› ä¸º<br>åœ¨parseå‡½æ•°å¼€å§‹å¤„ item = NewsItem() æ ¼å¼åŒ–ç±» æœ€åreturn item æäº¤item**
```python
item = TutorialItem()
å·´æ‹‰å·´æ‹‰å·´æ‹‰
return item
```
**æ–¹æ³•ä½¿ç”¨ï¼š**<br>

?> ä¹‹å‰ç”¨çš„<br>BeautifulSoup<br>
ç›¸å½“äºç°åœ¨çš„<br>title=response.css('title::text').get()<br>
è¿™é‡Œçš„ ::text åˆ™æ˜¯å®šä½åˆ°çš„æ ‡ç­¾ä¸­çš„textå±æ€§<br>

**æ³¨æ„ï¼š**<br>

?> scrapyæ—§ç‰ˆå†™æ³•ï¼š<br>
.extract_first()   å­—ç¬¦ä¸²<br>
.extract()   åˆ—è¡¨<br><br>
scrapyæ–°ç‰ˆå†™æ³•ï¼š<br>
.get()  å­—ç¬¦ä¸²<br>
.get_all()  åˆ—è¡¨<br>
```python
#ä¹‹å‰çš„ï¼š
from bs4 import BeautifulSoup
soup = BeautifulSoup(response.text,'lxml')
list_data=soup.select('#u1 a')
#ç°åœ¨çš„ï¼š
list_data=response.css("#u1 a::attr(href)").get()
```
**æ³¨æ„ï¼š**<br>

?>åœ¨çˆ¬è™«æ–‡ä»¶ä¸­æ‰€æœ‰è¦ç”¨åˆ°responseçš„éƒ½è¦åœ¨å‡½æ•°åé¢æ‹¬å·é‡Œé¢åŠ responseï¼Œå¦‚ï¼ˆè¿™é‡Œç¬¬ä¸€ä¸ªå‡½æ•°æ˜¯å‘èµ·è¯·æ±‚çš„ï¼Œä¸ç”¨åŠ responseï¼Œåé¢parseç›´æ¥ç”¨åˆ°äº†ç¬¬ä¸€ä¸ªç½‘é¡µï¼Œå°±è¦åŠ responseï¼‰ï¼š<br>
```python
    def start_requests(self):
        urls = [
            'http://www.baidu.com',
            'http://www.baidu.com',
        ]
        for url in urls:
            yield scrapy.Request(url=url, callback=self.parse)

    def parse(self, response):

        print(response.css('title::text').get())
        print(response.css("#u1 a::attr(href)").get())

        soup = BeautifulSoup(response.text,'lxml')
        print(soup.title)
```




<br><br><br><br>

# pipelineæ–‡ä»¶ â˜¢ï¸

## **å†™å…¥csvæ–‡ä»¶** â˜£ï¸
å…ˆå¯¼åŒ…ï¼š<br>
```python
from scrapy.exporters import CsvItemExporter
```
è¿™ä¸ªç±»é‡Œé¢æœ‰ä¸‰ä¸ªå‡½æ•°ï¼Œåˆ†åˆ«æ˜¯åˆ›å»ºï¼Œå†™å…¥ï¼Œå…³é—­ï¼Œå¦‚ä¸‹ï¼š
```python
class NewsPipelineCsv(object):

    def __init__(self):
        self.file = open('news_data.csv','wb')#åˆ›å»ºæ–‡ä»¶
        self.exporter = CsvItemExporter(self.file,encoding = 'utf-8')
        self.exporter.start_exporting()#å¯åŠ¨å†™å…¥
    
    def process_item(self, item, spider):
        self.exporter.export_item(item)
        return item

    def close_spider(self,spider):
        self.exporter.finish_exporting()#å…³é—­å†™å…¥
        self.file.close()#å…³é—­æ–‡ä»¶
```

?>ç¬¬ä¸€ä¸ªå‡½æ•°<br>é‡Œé¢__init__()<br>
self.exporter = JsonItemExporter(self.file,encoding = 'utf-8')<br>
JsonItemExporterå°±æ˜¯å¯¼çš„åŒ…<br>

?>åŸºæœ¬æ€è·¯ï¼šåˆ›å»ºæ–‡ä»¶-->ä¿®æ”¹ç¼–ç -->å¯åŠ¨å†™å…¥-->å†™å…¥itemï¼Œç„¶åæäº¤item-->å…³é—­å†™å…¥-->å…³é—­æ–‡ä»¶<br><br>


## **å†™å…¥jsonæ–‡ä»¶** â°
å…ˆå¯¼åŒ…ï¼š<br>
```python
from scrapy.exporters import JsonItemExporter
```

?>é™¤å»ç±»çš„åå­—å’Œå¯¼çš„åŒ…ä¸ä¸€æ ·ï¼Œå…¶ä»–å’Œcsvä¸€æ ·<br>

**å› ä¸ºæ¢è¡Œçš„______**<br>
?>åœ¨å†™å…¥çš„å‡½æ•°é‡Œé¢è¦æ·»åŠ ä¸€è¡Œ<br>
self.exporter.indent = 1<br>

**æ³¨æ„ï¼š**<br>
?>1ï¼šç±»çš„æ‹¬å·é‡Œé¢è¦åŠ å…¥objectï¼ˆä¹Ÿå¯ä¸åŠ ï¼‰<br>
2:åˆ›å»ºæ–‡ä»¶çš„æ—¶å€™æ˜¯'wb'å†™å…¥<br>

**cssæ‰¾å…ƒç´ çš„ä½¿ç”¨ï¼š**<br>
```python
title = response.css('title::text').extract_first()
å°±æ˜¯soupä¸­è¿™æ ·çš„ä½¿ç”¨ï¼š
title=soup.select_one('title').text

å½“è¦é€‰ä¸­å…ƒç´ çš„æ—¶å€™å°±æ˜¯è¿™æ ·çš„ä½¿ç”¨(å¤ä¹ bs4,Xpath,css åœ¨é€‰æ‹©ä¸Šçš„åŒºåˆ«)ï¼š
title = response.css('a.u-icn.u-icn-81.icn-add::attr(href)').extract_first()
ç›¸å½“äºbs4ï¼š
title=soup.select_one('a.u-icn.u-icn-81.icn-add')['href']
ç›¸å½“äºxPathï¼š
title=response.xpath("//div[@class='opt hshow']/a[1]/@href")

```


## **å†™å…¥æ•°æ®åº“** å¹²å·´çˆ¹ï¼ï¼ï¼ğŸŒ±

- å¯¼åŒ…ï¼š
```python
import pymysql
```

?>åŒæ ·ä¹Ÿæ˜¯ä¸‰ä¸ªå‡½æ•°ï¼Œä½†æ˜¯ä¸å†™å…¥csvå’Œjsonä¸åŒçš„æ˜¯ï¼š<br>
æ•°æ®åº“ç¬¬ä¸€ä¸ªæ˜¯åˆ›å»ºæ¸¸æ ‡ã€åˆ›å»ºè¿æ¥ï¼Œè€Œä¸æ˜¯åˆ›å»ºæ–‡ä»¶ï¼›<br>
ç¬¬äºŒä¸ªæ–‡ä»¶æ˜¯å†™å…¥æ•°æ®åº“ï¼Œè¦tryé‡Œé¢return itemï¼›<br>
ç¬¬ä¸‰ä¸ªæ–‡ä»¶æ˜¯å…³é—­è¿æ¥å’Œå…³é—­æ¸¸æ ‡ã€‚




<br><br><br><br>

# settingæ–‡ä»¶ ğŸ’ 


?>ROBOTSTXT_OBEY<br>
æ”¹æˆFalseï¼Œä¸ºä¸éµå¾ªè¿™ä¸ªåè®®<br><br><br>
æ‰¾åˆ°ITEM_PIPELINESé…ç½®ï¼Œ<br>
æŠŠpipelineæ–‡ä»¶é‡Œé¢çš„å®šä¹‰çš„ç±»çš„åå­—åŠ å…¥ï¼Œè°ƒèŠ‚ä¼˜å…ˆçº§ï¼ˆä¼˜å…ˆçº§1ä¸ºæœ€å¤§ä¼˜å…ˆçº§ï¼‰<br>
å¦‚å›¾æ‰€ç¤ºè®¾ç½®ä¸º300ï¼Œ301ï¼Œ302ï¼ˆå­—å…¸å½¢å¼ï¼‰
```python
ITEM_PIPELINES = {
   'news.pipelines.NewsPipelineCsv': 300,
   'news.pipelines.NewsPipelineJson': 301,
   'news.pipelines.NewsPipelineMysql': 302,
}
```

**æ³¨æ„ï¼š**<br>
?>pipelineæ–‡ä»¶é‡Œé¢æ²¡æœ‰çš„ç±»<br>
settingæ–‡ä»¶é‡Œé¢çš„ä¼˜å…ˆçº§è®¾ç½®é‡Œé¢ä¹Ÿä¸èƒ½æœ‰è¿™ä¸ªç±»çš„åå­—


<br><br><br><br>

?>å¦‚æœä¸è·³è¿›é“¾æ¥çˆ¬å–ï¼Œå°±åœ¨ç¬¬ä¸€ä¸ªé“¾æ¥çˆ¬å–ï¼Œåˆ™ï¼š


```python
æ–¹æ³•ä¸€ï¼š
ç›´æ¥åœ¨parseé‡Œé¢å†™ï¼Œresponseåˆ™æ˜¯è¯¥ç½‘é¡µçš„è¯·æ±‚
ç›´æ¥æäº¤ä¸€ä¸ªåˆ—è¡¨ï¼Œdictåé¢çš„åˆ™æ˜¯itemæ–‡ä»¶é‡Œé¢çš„å˜é‡


    def parse(self, response):
        title = response.css('div.ns_area.list ul li a::text').extract()
        list_data=[]
        for i in title:
            list_data.append(dict(title_data=i))
        return list_data
            
        urls = response.css('div.ns_area.list ul li a::attr(href)').extract()
        data_list=[]
        for url in urls:
            data_list.append(dict(url_data=url))
        return data_list


æ–¹æ³•äºŒï¼š
ä¸ç”¨æäº¤item
ç›´æ¥è¿”å›ä¸€ä¸ªå­—å…¸åˆ—è¡¨

å› ä¸ºåªèƒ½è¿”å›ä¸€ä¸ªå€¼ï¼Œæ‰€ä»¥åªèƒ½äºŒé€‰ä¸€

    def parse(self, response):
        # title = response.css('div.ns_area.list ul li a::text').extract()
        # list_data=[]
        # for i in title:
        #     list_data.append(dict(title_data=i))
        # return list_data
            
        urls = response.css('div.ns_area.list ul li a::attr(href)').extract()
        data_list=[]
        for url in urls:
            data_list.append(dict(url_data=url))
        return data_list
```


ğŸ’ªğŸ’ªğŸ’ªğŸ’ªğŸ’ªğŸ’ªğŸ’ªğŸ’ªğŸ’ªğŸ’ªğŸ’ªğŸ’ªğŸ’ªğŸ’ªğŸ’ª


